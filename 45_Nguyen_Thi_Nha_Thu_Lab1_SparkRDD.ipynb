{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khởi tạo SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tạo một RDD từ một collection\n",
    "### 1.1 Tạo một Python list gồm các số từ 1 đến 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb = list(range(1,101))\n",
    "numb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Tạo một RDD từ Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "numbRDD = sc.parallelize(numb)\n",
    "for i in numbRDD.take(10):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. In ra kiểu của `numbRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of RDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "from msilib import type_string\n",
    "\n",
    "\n",
    "print(\"The type of RDD is\",type(numbRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tạo một RDD từ một external dataset\n",
    "### 2.1. Tạo một RDD từ một file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ./README.md MapPartitionsRDD[3] at textFile at <unknown>:0\n",
      "\n",
      " ['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_computeFractionForSampleSize', '_defaultReducePartitions', '_id', '_is_barrier', '_jrdd', '_jrdd_deserializer', '_memory_limit', '_pickled', '_reserialize', '_to_java_object_rdd', 'aggregate', 'aggregateByKey', 'barrier', 'cache', 'cartesian', 'checkpoint', 'coalesce', 'cogroup', 'collect', 'collectAsMap', 'collectWithJobGroup', 'combineByKey', 'context', 'count', 'countApprox', 'countApproxDistinct', 'countByKey', 'countByValue', 'ctx', 'distinct', 'filter', 'first', 'flatMap', 'flatMapValues', 'fold', 'foldByKey', 'foreach', 'foreachPartition', 'fullOuterJoin', 'getCheckpointFile', 'getNumPartitions', 'getResourceProfile', 'getStorageLevel', 'glom', 'groupBy', 'groupByKey', 'groupWith', 'has_resource_profile', 'histogram', 'id', 'intersection', 'isCheckpointed', 'isEmpty', 'isLocallyCheckpointed', 'is_cached', 'is_checkpointed', 'join', 'keyBy', 'keys', 'leftOuterJoin', 'localCheckpoint', 'lookup', 'map', 'mapPartitions', 'mapPartitionsWithIndex', 'mapPartitionsWithSplit', 'mapValues', 'max', 'mean', 'meanApprox', 'min', 'name', 'partitionBy', 'partitioner', 'persist', 'pipe', 'randomSplit', 'reduce', 'reduceByKey', 'reduceByKeyLocally', 'repartition', 'repartitionAndSortWithinPartitions', 'rightOuterJoin', 'sample', 'sampleByKey', 'sampleStdev', 'sampleVariance', 'saveAsHadoopDataset', 'saveAsHadoopFile', 'saveAsNewAPIHadoopDataset', 'saveAsNewAPIHadoopFile', 'saveAsPickleFile', 'saveAsSequenceFile', 'saveAsTextFile', 'setName', 'sortBy', 'sortByKey', 'stats', 'stdev', 'subtract', 'subtractByKey', 'sum', 'sumApprox', 'take', 'takeOrdered', 'takeSample', 'toDebugString', 'toLocalIterator', 'top', 'treeAggregate', 'treeReduce', 'union', 'unpersist', 'values', 'variance', 'withResources', 'zip', 'zipWithIndex', 'zipWithUniqueId']\n"
     ]
    }
   ],
   "source": [
    "file_path = './README.md' # File readme của thư mục cài đặt Spark\n",
    "fileRDD = sc.textFile(file_path)\n",
    "print(\"\\n\", fileRDD)\n",
    "print(\"\\n\", dir(fileRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Kiểm tra kiểu của `fileRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file type of fileRDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phân hoạch dữ liệu\n",
    "### 3.1. Kiểm tra số partition của `fileRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in fileRDD is 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Tạo `fileRDD_part` từ `file_path` với 5 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Kiểm tra số partition của `fileRDD_part`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in fileRDD_part is 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Map and Collect\n",
    "### 4.1. Tạo một map() transformation để tính lũy thừa bậc 3 của các phần tử trong `numbRDD`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubedRDD = numbRDD.map(lambda x: x*x*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Collect kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000, 1331, 1728, 2197, 2744, 3375, 4096, 4913, 5832, 6859, 8000, 9261, 10648, 12167, 13824, 15625, 17576, 19683, 21952, 24389, 27000, 29791, 32768, 35937, 39304, 42875, 46656, 50653, 54872, 59319, 64000, 68921, 74088, 79507, 85184, 91125, 97336, 103823, 110592, 117649, 125000, 132651, 140608, 148877, 157464, 166375, 175616, 185193, 195112, 205379, 216000, 226981, 238328, 250047, 262144, 274625, 287496, 300763, 314432, 328509, 343000, 357911, 373248, 389017, 405224, 421875, 438976, 456533, 474552, 493039, 512000, 531441, 551368, 571787, 592704, 614125, 636056, 658503, 681472, 704969, 729000, 753571, 778688, 804357, 830584, 857375, 884736, 912673, 941192, 970299, 1000000]\n"
     ]
    }
   ],
   "source": [
    "numbers_all = cubedRDD.collect()\n",
    "print(numbers_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. In ra các phần tử từ `numbers_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n",
      "1000\n",
      "1331\n",
      "1728\n",
      "2197\n",
      "2744\n",
      "3375\n",
      "4096\n",
      "4913\n",
      "5832\n",
      "6859\n",
      "8000\n",
      "9261\n",
      "10648\n",
      "12167\n",
      "13824\n",
      "15625\n",
      "17576\n",
      "19683\n",
      "21952\n",
      "24389\n",
      "27000\n",
      "29791\n",
      "32768\n",
      "35937\n",
      "39304\n",
      "42875\n",
      "46656\n",
      "50653\n",
      "54872\n",
      "59319\n",
      "64000\n",
      "68921\n",
      "74088\n",
      "79507\n",
      "85184\n",
      "91125\n",
      "97336\n",
      "103823\n",
      "110592\n",
      "117649\n",
      "125000\n",
      "132651\n",
      "140608\n",
      "148877\n",
      "157464\n",
      "166375\n",
      "175616\n",
      "185193\n",
      "195112\n",
      "205379\n",
      "216000\n",
      "226981\n",
      "238328\n",
      "250047\n",
      "262144\n",
      "274625\n",
      "287496\n",
      "300763\n",
      "314432\n",
      "328509\n",
      "343000\n",
      "357911\n",
      "373248\n",
      "389017\n",
      "405224\n",
      "421875\n",
      "438976\n",
      "456533\n",
      "474552\n",
      "493039\n",
      "512000\n",
      "531441\n",
      "551368\n",
      "571787\n",
      "592704\n",
      "614125\n",
      "636056\n",
      "658503\n",
      "681472\n",
      "704969\n",
      "729000\n",
      "753571\n",
      "778688\n",
      "804357\n",
      "830584\n",
      "857375\n",
      "884736\n",
      "912673\n",
      "941192\n",
      "970299\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "for numb in numbers_all:\n",
    "\tprint(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter and Count\n",
    "### 5.1 Áp dụng filter trên `fileRDD` để chọn ra những dòng có từ `Spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Cho biết có bao nhiêu dòng từ `fileRDD`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of line with the keyword Spark is 19\n"
     ]
    }
   ],
   "source": [
    "print(\"The total number of line with the keyword Spark is\",fileRDD_filter.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. In 5 dòng đầu của `fileRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apache Spark\n",
      "Spark is a unified analytics engine for large-scale data processing. It provides\n",
      "rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n",
      "[![PySpark Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)\n",
      "You can find the latest Spark documentation, including a programming\n"
     ]
    }
   ],
   "source": [
    "for line in fileRDD_filter.take(5): \n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ReduceBykey and Collect  \n",
    "### 6.1. Tạo một `PairRDD` tên `Rdd` như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (3, 6), (4, 5)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
    "Rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Áp dụng `reduceByKey()` operation trên Rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10), (4, 5)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
    "Rdd_Reduced.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Duyệt và in ra kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 4 has 5 Counts\n"
     ]
    }
   ],
   "source": [
    "for num in Rdd_Reduced.collect(): \n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SortByKey and Collect\n",
    "### 7.1. Sắp xếp `Rdd_Reduced` theo khóa giảm dần"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Duyệt và in ra kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 1 has 2 Counts\n"
     ]
    }
   ],
   "source": [
    "for num in Rdd_Reduced_Sort.collect():\n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. CountingBykeys\n",
    "### 8.1. Đếm số key khác nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = Rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Kiểm tra kiểu của biến `total`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of total is <class 'collections.defaultdict'>\n"
     ]
    }
   ],
   "source": [
    "print(\"The type of total is\", type(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Duyệt và in ra kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key 1 has 1 counts\n",
      "key 3 has 2 counts\n",
      "key 4 has 1 counts\n"
     ]
    }
   ],
   "source": [
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tạo biến `baseRDD` từ file và thực hiện các transform\n",
    "### 9.1 Tạo biến `baseRDD` từ `file_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./README.md MapPartitionsRDD[24] at textFile at <unknown>:0\n"
     ]
    }
   ],
   "source": [
    "file_path = './README.md'\n",
    "baseRDD = sc.textFile(file_path)\n",
    "print(baseRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Biến đổi các dòng trong `baseRDD` thành các từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Apache', 'Spark', 'Spark', 'is', 'a', 'unified', 'analytics', 'engine', 'for', 'large-scale', 'data', 'processing.', 'It', 'provides', 'high-level', 'APIs', 'in', 'Scala,', 'Java,', 'Python,', 'and', 'R,', 'and', 'an', 'optimized', 'engine', 'that', 'supports', 'general', 'computation', 'graphs', 'for', 'data', 'analysis.', 'It', 'also', 'supports', 'a', 'rich', 'set', 'of', 'higher-level', 'tools', 'including', 'Spark', 'SQL', 'for', 'SQL', 'and', 'DataFrames,', 'MLlib', 'for', 'machine', 'learning,', 'GraphX', 'for', 'graph', 'processing,', 'and', 'Structured', 'Streaming', 'for', 'stream', 'processing.', '<https://spark.apache.org/>', '[![Jenkins', 'Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)', '[![AppVeyor', 'Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)', '[![PySpark', 'Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)', '##', 'Online', 'Documentation', 'You', 'can', 'find', 'the', 'latest', 'Spark', 'documentation,', 'including', 'a', 'programming', 'guide,', 'on', 'the', '[project', 'web', 'page](https://spark.apache.org/documentation.html).', 'This', 'README', 'file', 'only', 'contains', 'basic', 'setup', 'instructions.', '##', 'Building', 'Spark', 'Spark', 'is', 'built', 'using', '[Apache', 'Maven](https://maven.apache.org/).', 'To', 'build', 'Spark', 'and', 'its', 'example', 'programs,', 'run:', './build/mvn', '-DskipTests', 'clean', 'package', '(You', 'do', 'not', 'need', 'to', 'do', 'this', 'if', 'you', 'downloaded', 'a', 'pre-built', 'package.)', 'More', 'detailed', 'documentation', 'is', 'available', 'from', 'the', 'project', 'site,', 'at', '[\"Building', 'Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 'For', 'general', 'development', 'tips,', 'including', 'info', 'on', 'developing', 'Spark', 'using', 'an', 'IDE,', 'see', '[\"Useful', 'Developer', 'Tools\"](https://spark.apache.org/developer-tools.html).', '##', 'Interactive', 'Scala', 'Shell', 'The', 'easiest', 'way', 'to', 'start', 'using', 'Spark', 'is', 'through', 'the', 'Scala', 'shell:', './bin/spark-shell', 'Try', 'the', 'following', 'command,', 'which', 'should', 'return', '1,000,000,000:', 'scala>', 'spark.range(1000', '*', '1000', '*', '1000).count()', '##', 'Interactive', 'Python', 'Shell', 'Alternatively,', 'if', 'you', 'prefer', 'Python,', 'you', 'can', 'use', 'the', 'Python', 'shell:', './bin/pyspark', 'And', 'run', 'the', 'following', 'command,', 'which', 'should', 'also', 'return', '1,000,000,000:', '>>>', 'spark.range(1000', '*', '1000', '*', '1000).count()', '##', 'Example', 'Programs', 'Spark', 'also', 'comes', 'with', 'several', 'sample', 'programs', 'in', 'the', '`examples`', 'directory.', 'To', 'run', 'one', 'of', 'them,', 'use', '`./bin/run-example', '<class>', '[params]`.', 'For', 'example:', './bin/run-example', 'SparkPi', 'will', 'run', 'the', 'Pi', 'example', 'locally.', 'You', 'can', 'set', 'the', 'MASTER', 'environment', 'variable', 'when', 'running', 'examples', 'to', 'submit', 'examples', 'to', 'a', 'cluster.', 'This', 'can', 'be', 'a', 'mesos://', 'or', 'spark://', 'URL,', '\"yarn\"', 'to', 'run', 'on', 'YARN,', 'and', '\"local\"', 'to', 'run', 'locally', 'with', 'one', 'thread,', 'or', '\"local[N]\"', 'to', 'run', 'locally', 'with', 'N', 'threads.', 'You', 'can', 'also', 'use', 'an', 'abbreviated', 'class', 'name', 'if', 'the', 'class', 'is', 'in', 'the', '`examples`', 'package.', 'For', 'instance:', 'MASTER=spark://host:7077', './bin/run-example', 'SparkPi', 'Many', 'of', 'the', 'example', 'programs', 'print', 'usage', 'help', 'if', 'no', 'params', 'are', 'given.', '##', 'Running', 'Tests', 'Testing', 'first', 'requires', '[building', 'Spark](#building-spark).', 'Once', 'Spark', 'is', 'built,', 'tests', 'can', 'be', 'run', 'using:', './dev/run-tests', 'Please', 'see', 'the', 'guidance', 'on', 'how', 'to', '[run', 'tests', 'for', 'a', 'module,', 'or', 'individual', 'tests](https://spark.apache.org/developer-tools.html#individual-tests).', 'There', 'is', 'also', 'a', 'Kubernetes', 'integration', 'test,', 'see', 'resource-managers/kubernetes/integration-tests/README.md', '##', 'A', 'Note', 'About', 'Hadoop', 'Versions', 'Spark', 'uses', 'the', 'Hadoop', 'core', 'library', 'to', 'talk', 'to', 'HDFS', 'and', 'other', 'Hadoop-supported', 'storage', 'systems.', 'Because', 'the', 'protocols', 'have', 'changed', 'in', 'different', 'versions', 'of', 'Hadoop,', 'you', 'must', 'build', 'Spark', 'against', 'the', 'same', 'version', 'that', 'your', 'cluster', 'runs.', 'Please', 'refer', 'to', 'the', 'build', 'documentation', 'at', '[\"Specifying', 'the', 'Hadoop', 'Version', 'and', 'Enabling', 'YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 'for', 'detailed', 'guidance', 'on', 'building', 'for', 'a', 'particular', 'distribution', 'of', 'Hadoop,', 'including', 'building', 'for', 'particular', 'Hive', 'and', 'Hive', 'Thriftserver', 'distributions.', '##', 'Configuration', 'Please', 'refer', 'to', 'the', '[Configuration', 'Guide](https://spark.apache.org/docs/latest/configuration.html)', 'in', 'the', 'online', 'documentation', 'for', 'an', 'overview', 'on', 'how', 'to', 'configure', 'Spark.', '##', 'Contributing', 'Please', 'review', 'the', '[Contribution', 'to', 'Spark', 'guide](https://spark.apache.org/contributing.html)', 'for', 'information', 'on', 'how', 'to', 'get', 'started', 'contributing', 'to', 'the', 'project.']\n"
     ]
    }
   ],
   "source": [
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "print(splitRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Đếm tổng số từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in splitRDD: 495\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Loại bỏ stop word và áp dụng reduce opperator trên tập dữ liệu\n",
    "### 10.1. Chuyển các từ sang dạng chữ thường (lower case) và loại bỏ các stop words từ biến `stop_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'unified',\n",
       " 'analytics',\n",
       " 'engine',\n",
       " 'for',\n",
       " 'large-scale',\n",
       " 'data',\n",
       " 'processing.',\n",
       " 'provides',\n",
       " 'high-level',\n",
       " 'APIs',\n",
       " 'in',\n",
       " 'Scala,',\n",
       " 'Java,',\n",
       " 'Python,',\n",
       " 'and',\n",
       " 'R,',\n",
       " 'and',\n",
       " 'optimized',\n",
       " 'engine',\n",
       " 'supports',\n",
       " 'general',\n",
       " 'computation',\n",
       " 'graphs',\n",
       " 'for',\n",
       " 'data',\n",
       " 'analysis.',\n",
       " 'also',\n",
       " 'supports',\n",
       " 'rich',\n",
       " 'set',\n",
       " 'of',\n",
       " 'higher-level',\n",
       " 'tools',\n",
       " 'including',\n",
       " 'Spark',\n",
       " 'SQL',\n",
       " 'for',\n",
       " 'SQL',\n",
       " 'and',\n",
       " 'DataFrames,',\n",
       " 'MLlib',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning,',\n",
       " 'GraphX',\n",
       " 'for',\n",
       " 'graph',\n",
       " 'processing,',\n",
       " 'and',\n",
       " 'Structured',\n",
       " 'Streaming',\n",
       " 'for',\n",
       " 'stream',\n",
       " 'processing.',\n",
       " '<https://spark.apache.org/>',\n",
       " '[![Jenkins',\n",
       " 'Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)',\n",
       " '[![AppVeyor',\n",
       " 'Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       " '[![PySpark',\n",
       " 'Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)',\n",
       " '##',\n",
       " 'Online',\n",
       " 'Documentation',\n",
       " 'You',\n",
       " 'can',\n",
       " 'find',\n",
       " 'latest',\n",
       " 'Spark',\n",
       " 'documentation,',\n",
       " 'including',\n",
       " 'programming',\n",
       " 'guide,',\n",
       " 'on',\n",
       " '[project',\n",
       " 'web',\n",
       " 'page](https://spark.apache.org/documentation.html).',\n",
       " 'README',\n",
       " 'file',\n",
       " 'only',\n",
       " 'contains',\n",
       " 'basic',\n",
       " 'setup',\n",
       " 'instructions.',\n",
       " '##',\n",
       " 'Building',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'built',\n",
       " 'using',\n",
       " '[Apache',\n",
       " 'Maven](https://maven.apache.org/).',\n",
       " 'To',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'and',\n",
       " 'its',\n",
       " 'example',\n",
       " 'programs,',\n",
       " 'run:',\n",
       " './build/mvn',\n",
       " '-DskipTests',\n",
       " 'clean',\n",
       " 'package',\n",
       " '(You',\n",
       " 'do',\n",
       " 'need',\n",
       " 'to',\n",
       " 'do',\n",
       " 'you',\n",
       " 'downloaded',\n",
       " 'pre-built',\n",
       " 'package.)',\n",
       " 'More',\n",
       " 'detailed',\n",
       " 'documentation',\n",
       " 'available',\n",
       " 'from',\n",
       " 'project',\n",
       " 'site,',\n",
       " 'at',\n",
       " '[\"Building',\n",
       " 'Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For',\n",
       " 'general',\n",
       " 'development',\n",
       " 'tips,',\n",
       " 'including',\n",
       " 'info',\n",
       " 'on',\n",
       " 'developing',\n",
       " 'Spark',\n",
       " 'using',\n",
       " 'IDE,',\n",
       " 'see',\n",
       " '[\"Useful',\n",
       " 'Developer',\n",
       " 'Tools\"](https://spark.apache.org/developer-tools.html).',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Scala',\n",
       " 'Shell',\n",
       " 'easiest',\n",
       " 'way',\n",
       " 'to',\n",
       " 'start',\n",
       " 'using',\n",
       " 'Spark',\n",
       " 'through',\n",
       " 'Scala',\n",
       " 'shell:',\n",
       " './bin/spark-shell',\n",
       " 'Try',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'return',\n",
       " '1,000,000,000:',\n",
       " 'scala>',\n",
       " 'spark.range(1000',\n",
       " '*',\n",
       " '1000',\n",
       " '*',\n",
       " '1000).count()',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Python',\n",
       " 'Shell',\n",
       " 'Alternatively,',\n",
       " 'you',\n",
       " 'prefer',\n",
       " 'Python,',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'Python',\n",
       " 'shell:',\n",
       " './bin/pyspark',\n",
       " 'And',\n",
       " 'run',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'also',\n",
       " 'return',\n",
       " '1,000,000,000:',\n",
       " '>>>',\n",
       " 'spark.range(1000',\n",
       " '*',\n",
       " '1000',\n",
       " '*',\n",
       " '1000).count()',\n",
       " '##',\n",
       " 'Example',\n",
       " 'Programs',\n",
       " 'Spark',\n",
       " 'also',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'several',\n",
       " 'sample',\n",
       " 'programs',\n",
       " 'in',\n",
       " '`examples`',\n",
       " 'directory.',\n",
       " 'To',\n",
       " 'run',\n",
       " 'one',\n",
       " 'of',\n",
       " 'them,',\n",
       " 'use',\n",
       " '`./bin/run-example',\n",
       " '<class>',\n",
       " '[params]`.',\n",
       " 'For',\n",
       " 'example:',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " 'will',\n",
       " 'run',\n",
       " 'Pi',\n",
       " 'example',\n",
       " 'locally.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'set',\n",
       " 'MASTER',\n",
       " 'environment',\n",
       " 'variable',\n",
       " 'when',\n",
       " 'running',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'cluster.',\n",
       " 'can',\n",
       " 'mesos://',\n",
       " 'spark://',\n",
       " 'URL,',\n",
       " '\"yarn\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'on',\n",
       " 'YARN,',\n",
       " 'and',\n",
       " '\"local\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'one',\n",
       " 'thread,',\n",
       " '\"local[N]\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'N',\n",
       " 'threads.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'use',\n",
       " 'abbreviated',\n",
       " 'class',\n",
       " 'name',\n",
       " 'class',\n",
       " 'in',\n",
       " '`examples`',\n",
       " 'package.',\n",
       " 'For',\n",
       " 'instance:',\n",
       " 'MASTER=spark://host:7077',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " 'Many',\n",
       " 'of',\n",
       " 'example',\n",
       " 'programs',\n",
       " 'print',\n",
       " 'usage',\n",
       " 'help',\n",
       " 'no',\n",
       " 'params',\n",
       " 'are',\n",
       " 'given.',\n",
       " '##',\n",
       " 'Running',\n",
       " 'Tests',\n",
       " 'Testing',\n",
       " 'first',\n",
       " 'requires',\n",
       " '[building',\n",
       " 'Spark](#building-spark).',\n",
       " 'Once',\n",
       " 'Spark',\n",
       " 'built,',\n",
       " 'tests',\n",
       " 'can',\n",
       " 'run',\n",
       " 'using:',\n",
       " './dev/run-tests',\n",
       " 'Please',\n",
       " 'see',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " '[run',\n",
       " 'tests',\n",
       " 'for',\n",
       " 'module,',\n",
       " 'individual',\n",
       " 'tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       " 'There',\n",
       " 'also',\n",
       " 'Kubernetes',\n",
       " 'integration',\n",
       " 'test,',\n",
       " 'see',\n",
       " 'resource-managers/kubernetes/integration-tests/README.md',\n",
       " '##',\n",
       " 'Note',\n",
       " 'About',\n",
       " 'Hadoop',\n",
       " 'Versions',\n",
       " 'Spark',\n",
       " 'uses',\n",
       " 'Hadoop',\n",
       " 'core',\n",
       " 'library',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'HDFS',\n",
       " 'and',\n",
       " 'other',\n",
       " 'Hadoop-supported',\n",
       " 'storage',\n",
       " 'systems.',\n",
       " 'Because',\n",
       " 'protocols',\n",
       " 'have',\n",
       " 'changed',\n",
       " 'in',\n",
       " 'different',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'you',\n",
       " 'must',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'against',\n",
       " 'same',\n",
       " 'version',\n",
       " 'your',\n",
       " 'cluster',\n",
       " 'runs.',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'build',\n",
       " 'documentation',\n",
       " 'at',\n",
       " '[\"Specifying',\n",
       " 'Hadoop',\n",
       " 'Version',\n",
       " 'and',\n",
       " 'Enabling',\n",
       " 'YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       " 'for',\n",
       " 'detailed',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'building',\n",
       " 'for',\n",
       " 'particular',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'including',\n",
       " 'building',\n",
       " 'for',\n",
       " 'particular',\n",
       " 'Hive',\n",
       " 'and',\n",
       " 'Hive',\n",
       " 'Thriftserver',\n",
       " 'distributions.',\n",
       " '##',\n",
       " 'Configuration',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " '[Configuration',\n",
       " 'Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in',\n",
       " 'online',\n",
       " 'documentation',\n",
       " 'for',\n",
       " 'overview',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'configure',\n",
       " 'Spark.',\n",
       " '##',\n",
       " 'Contributing',\n",
       " 'Please',\n",
       " 'review',\n",
       " '[Contribution',\n",
       " 'to',\n",
       " 'Spark',\n",
       " 'guide](https://spark.apache.org/contributing.html)',\n",
       " 'for',\n",
       " 'information',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'get',\n",
       " 'started',\n",
       " 'contributing',\n",
       " 'to',\n",
       " 'project.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = ['a', 'an', 'the', 'is', 'be', 'was', 'were', 'it', 'this', 'that', 'but', 'if', 'or', 'not']\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "splitRDD_no_stop.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2. Tạo các tuple có dạng (word, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('Apache', 1), ('Spark', 1), ('Spark', 1), ('unified', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "splitRDD_no_stop_words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3. Đếm tần số của mỗi từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark', 14),\n",
       " ('unified', 1),\n",
       " ('analytics', 1),\n",
       " ('engine', 2),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('in', 5),\n",
       " ('Scala,', 1),\n",
       " ('Java,', 1),\n",
       " ('optimized', 1),\n",
       " ('supports', 2),\n",
       " ('computation', 1),\n",
       " ('analysis.', 1),\n",
       " ('set', 2),\n",
       " ('of', 5),\n",
       " ('tools', 1),\n",
       " ('SQL', 2),\n",
       " ('MLlib', 1),\n",
       " ('machine', 1),\n",
       " ('learning,', 1),\n",
       " ('GraphX', 1),\n",
       " ('graph', 1),\n",
       " ('processing,', 1),\n",
       " ('Structured', 1),\n",
       " ('<https://spark.apache.org/>', 1),\n",
       " ('Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       "  1),\n",
       " ('Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)',\n",
       "  1),\n",
       " ('Documentation', 1),\n",
       " ('latest', 1),\n",
       " ('programming', 1),\n",
       " ('guide,', 1),\n",
       " ('[project', 1),\n",
       " ('page](https://spark.apache.org/documentation.html).', 1),\n",
       " ('README', 1),\n",
       " ('only', 1),\n",
       " ('basic', 1),\n",
       " ('instructions.', 1),\n",
       " ('Building', 1),\n",
       " ('using', 3),\n",
       " ('[Apache', 1),\n",
       " ('run:', 1),\n",
       " ('do', 2),\n",
       " ('downloaded', 1),\n",
       " ('documentation', 3),\n",
       " ('project', 1),\n",
       " ('site,', 1),\n",
       " ('at', 2),\n",
       " ('Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('development', 1),\n",
       " ('tips,', 1),\n",
       " ('developing', 1),\n",
       " ('IDE,', 1),\n",
       " ('[\"Useful', 1),\n",
       " ('Developer', 1),\n",
       " ('Interactive', 2),\n",
       " ('Shell', 2),\n",
       " ('way', 1),\n",
       " ('start', 1),\n",
       " ('Try', 1),\n",
       " ('following', 2),\n",
       " ('scala>', 1),\n",
       " ('spark.range(1000', 2),\n",
       " ('*', 4),\n",
       " ('1000).count()', 2),\n",
       " ('Python', 2),\n",
       " ('Alternatively,', 1),\n",
       " ('use', 3),\n",
       " ('And', 1),\n",
       " ('run', 7),\n",
       " ('Example', 1),\n",
       " ('several', 1),\n",
       " ('programs', 2),\n",
       " ('them,', 1),\n",
       " ('`./bin/run-example', 1),\n",
       " ('[params]`.', 1),\n",
       " ('example:', 1),\n",
       " ('./bin/run-example', 2),\n",
       " ('SparkPi', 2),\n",
       " ('variable', 1),\n",
       " ('when', 1),\n",
       " ('examples', 2),\n",
       " ('spark://', 1),\n",
       " ('URL,', 1),\n",
       " ('YARN,', 1),\n",
       " ('\"local\"', 1),\n",
       " ('locally', 2),\n",
       " ('N', 1),\n",
       " ('abbreviated', 1),\n",
       " ('class', 2),\n",
       " ('name', 1),\n",
       " ('package.', 1),\n",
       " ('instance:', 1),\n",
       " ('print', 1),\n",
       " ('usage', 1),\n",
       " ('help', 1),\n",
       " ('no', 1),\n",
       " ('params', 1),\n",
       " ('are', 1),\n",
       " ('Testing', 1),\n",
       " ('Spark](#building-spark).', 1),\n",
       " ('Once', 1),\n",
       " ('built,', 1),\n",
       " ('tests', 2),\n",
       " ('using:', 1),\n",
       " ('./dev/run-tests', 1),\n",
       " ('Please', 4),\n",
       " ('guidance', 2),\n",
       " ('module,', 1),\n",
       " ('individual', 1),\n",
       " ('integration', 1),\n",
       " ('test,', 1),\n",
       " ('Note', 1),\n",
       " ('About', 1),\n",
       " ('uses', 1),\n",
       " ('library', 1),\n",
       " ('HDFS', 1),\n",
       " ('other', 1),\n",
       " ('Hadoop-supported', 1),\n",
       " ('storage', 1),\n",
       " ('systems.', 1),\n",
       " ('Because', 1),\n",
       " ('have', 1),\n",
       " ('changed', 1),\n",
       " ('different', 1),\n",
       " ('versions', 1),\n",
       " ('Hadoop,', 2),\n",
       " ('must', 1),\n",
       " ('against', 1),\n",
       " ('version', 1),\n",
       " ('refer', 2),\n",
       " ('YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       "  1),\n",
       " ('particular', 2),\n",
       " ('distribution', 1),\n",
       " ('Hive', 2),\n",
       " ('Thriftserver', 1),\n",
       " ('distributions.', 1),\n",
       " ('[Configuration', 1),\n",
       " ('online', 1),\n",
       " ('overview', 1),\n",
       " ('configure', 1),\n",
       " ('Spark.', 1),\n",
       " ('Contributing', 1),\n",
       " ('guide](https://spark.apache.org/contributing.html)', 1),\n",
       " ('started', 1),\n",
       " ('contributing', 1),\n",
       " ('project.', 1),\n",
       " ('for', 12),\n",
       " ('large-scale', 1),\n",
       " ('data', 2),\n",
       " ('processing.', 2),\n",
       " ('Python,', 2),\n",
       " ('and', 9),\n",
       " ('R,', 1),\n",
       " ('general', 2),\n",
       " ('graphs', 1),\n",
       " ('also', 5),\n",
       " ('rich', 1),\n",
       " ('higher-level', 1),\n",
       " ('including', 4),\n",
       " ('DataFrames,', 1),\n",
       " ('Streaming', 1),\n",
       " ('stream', 1),\n",
       " ('[![Jenkins', 1),\n",
       " ('Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)',\n",
       "  1),\n",
       " ('[![AppVeyor', 1),\n",
       " ('[![PySpark', 1),\n",
       " ('##', 9),\n",
       " ('Online', 1),\n",
       " ('You', 3),\n",
       " ('can', 6),\n",
       " ('find', 1),\n",
       " ('documentation,', 1),\n",
       " ('on', 7),\n",
       " ('web', 1),\n",
       " ('file', 1),\n",
       " ('contains', 1),\n",
       " ('setup', 1),\n",
       " ('built', 1),\n",
       " ('Maven](https://maven.apache.org/).', 1),\n",
       " ('To', 2),\n",
       " ('build', 3),\n",
       " ('its', 1),\n",
       " ('example', 3),\n",
       " ('programs,', 1),\n",
       " ('./build/mvn', 1),\n",
       " ('-DskipTests', 1),\n",
       " ('clean', 1),\n",
       " ('package', 1),\n",
       " ('(You', 1),\n",
       " ('need', 1),\n",
       " ('to', 16),\n",
       " ('you', 4),\n",
       " ('pre-built', 1),\n",
       " ('package.)', 1),\n",
       " ('More', 1),\n",
       " ('detailed', 2),\n",
       " ('available', 1),\n",
       " ('from', 1),\n",
       " ('[\"Building', 1),\n",
       " ('For', 3),\n",
       " ('info', 1),\n",
       " ('see', 3),\n",
       " ('Tools\"](https://spark.apache.org/developer-tools.html).', 1),\n",
       " ('Scala', 2),\n",
       " ('easiest', 1),\n",
       " ('through', 1),\n",
       " ('shell:', 2),\n",
       " ('./bin/spark-shell', 1),\n",
       " ('command,', 2),\n",
       " ('which', 2),\n",
       " ('should', 2),\n",
       " ('return', 2),\n",
       " ('1,000,000,000:', 2),\n",
       " ('1000', 2),\n",
       " ('prefer', 1),\n",
       " ('./bin/pyspark', 1),\n",
       " ('>>>', 1),\n",
       " ('Programs', 1),\n",
       " ('comes', 1),\n",
       " ('with', 3),\n",
       " ('sample', 1),\n",
       " ('`examples`', 2),\n",
       " ('directory.', 1),\n",
       " ('one', 2),\n",
       " ('<class>', 1),\n",
       " ('will', 1),\n",
       " ('Pi', 1),\n",
       " ('locally.', 1),\n",
       " ('MASTER', 1),\n",
       " ('environment', 1),\n",
       " ('running', 1),\n",
       " ('submit', 1),\n",
       " ('cluster.', 1),\n",
       " ('mesos://', 1),\n",
       " ('\"yarn\"', 1),\n",
       " ('thread,', 1),\n",
       " ('\"local[N]\"', 1),\n",
       " ('threads.', 1),\n",
       " ('MASTER=spark://host:7077', 1),\n",
       " ('Many', 1),\n",
       " ('given.', 1),\n",
       " ('Running', 1),\n",
       " ('Tests', 1),\n",
       " ('first', 1),\n",
       " ('requires', 1),\n",
       " ('[building', 1),\n",
       " ('how', 3),\n",
       " ('[run', 1),\n",
       " ('tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       "  1),\n",
       " ('There', 1),\n",
       " ('Kubernetes', 1),\n",
       " ('resource-managers/kubernetes/integration-tests/README.md', 1),\n",
       " ('Hadoop', 3),\n",
       " ('Versions', 1),\n",
       " ('core', 1),\n",
       " ('talk', 1),\n",
       " ('protocols', 1),\n",
       " ('same', 1),\n",
       " ('your', 1),\n",
       " ('cluster', 1),\n",
       " ('runs.', 1),\n",
       " ('[\"Specifying', 1),\n",
       " ('Version', 1),\n",
       " ('Enabling', 1),\n",
       " ('building', 2),\n",
       " ('Configuration', 1),\n",
       " ('Guide](https://spark.apache.org/docs/latest/configuration.html)', 1),\n",
       " ('review', 1),\n",
       " ('[Contribution', 1),\n",
       " ('information', 1),\n",
       " ('get', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "resultRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. In tần số của mỗi từ\n",
    "### 11.1. Hiển thị 10 từ đầu và tần số của nó trong `resultRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#', 1)\n",
      "('Apache', 1)\n",
      "('Spark', 14)\n",
      "('unified', 1)\n",
      "('analytics', 1)\n",
      "('engine', 2)\n",
      "('provides', 1)\n",
      "('high-level', 1)\n",
      "('APIs', 1)\n",
      "('in', 5)\n"
     ]
    }
   ],
   "source": [
    "for word in resultRDD.take(10):\n",
    "\tprint(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Hoán đổi giữa key và value trong `resultRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '#'),\n",
       " (1, 'Apache'),\n",
       " (14, 'Spark'),\n",
       " (1, 'unified'),\n",
       " (1, 'analytics'),\n",
       " (2, 'engine'),\n",
       " (1, 'provides'),\n",
       " (1, 'high-level'),\n",
       " (1, 'APIs'),\n",
       " (5, 'in'),\n",
       " (1, 'Scala,'),\n",
       " (1, 'Java,'),\n",
       " (1, 'optimized'),\n",
       " (2, 'supports'),\n",
       " (1, 'computation'),\n",
       " (1, 'analysis.'),\n",
       " (2, 'set'),\n",
       " (5, 'of'),\n",
       " (1, 'tools'),\n",
       " (2, 'SQL'),\n",
       " (1, 'MLlib'),\n",
       " (1, 'machine'),\n",
       " (1, 'learning,'),\n",
       " (1, 'GraphX'),\n",
       " (1, 'graph'),\n",
       " (1, 'processing,'),\n",
       " (1, 'Structured'),\n",
       " (1, '<https://spark.apache.org/>'),\n",
       " (1,\n",
       "  'Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)'),\n",
       " (1,\n",
       "  'Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)'),\n",
       " (1, 'Documentation'),\n",
       " (1, 'latest'),\n",
       " (1, 'programming'),\n",
       " (1, 'guide,'),\n",
       " (1, '[project'),\n",
       " (1, 'page](https://spark.apache.org/documentation.html).'),\n",
       " (1, 'README'),\n",
       " (1, 'only'),\n",
       " (1, 'basic'),\n",
       " (1, 'instructions.'),\n",
       " (1, 'Building'),\n",
       " (3, 'using'),\n",
       " (1, '[Apache'),\n",
       " (1, 'run:'),\n",
       " (2, 'do'),\n",
       " (1, 'downloaded'),\n",
       " (3, 'documentation'),\n",
       " (1, 'project'),\n",
       " (1, 'site,'),\n",
       " (2, 'at'),\n",
       " (1, 'Spark\"](https://spark.apache.org/docs/latest/building-spark.html).'),\n",
       " (1, 'development'),\n",
       " (1, 'tips,'),\n",
       " (1, 'developing'),\n",
       " (1, 'IDE,'),\n",
       " (1, '[\"Useful'),\n",
       " (1, 'Developer'),\n",
       " (2, 'Interactive'),\n",
       " (2, 'Shell'),\n",
       " (1, 'way'),\n",
       " (1, 'start'),\n",
       " (1, 'Try'),\n",
       " (2, 'following'),\n",
       " (1, 'scala>'),\n",
       " (2, 'spark.range(1000'),\n",
       " (4, '*'),\n",
       " (2, '1000).count()'),\n",
       " (2, 'Python'),\n",
       " (1, 'Alternatively,'),\n",
       " (3, 'use'),\n",
       " (1, 'And'),\n",
       " (7, 'run'),\n",
       " (1, 'Example'),\n",
       " (1, 'several'),\n",
       " (2, 'programs'),\n",
       " (1, 'them,'),\n",
       " (1, '`./bin/run-example'),\n",
       " (1, '[params]`.'),\n",
       " (1, 'example:'),\n",
       " (2, './bin/run-example'),\n",
       " (2, 'SparkPi'),\n",
       " (1, 'variable'),\n",
       " (1, 'when'),\n",
       " (2, 'examples'),\n",
       " (1, 'spark://'),\n",
       " (1, 'URL,'),\n",
       " (1, 'YARN,'),\n",
       " (1, '\"local\"'),\n",
       " (2, 'locally'),\n",
       " (1, 'N'),\n",
       " (1, 'abbreviated'),\n",
       " (2, 'class'),\n",
       " (1, 'name'),\n",
       " (1, 'package.'),\n",
       " (1, 'instance:'),\n",
       " (1, 'print'),\n",
       " (1, 'usage'),\n",
       " (1, 'help'),\n",
       " (1, 'no'),\n",
       " (1, 'params'),\n",
       " (1, 'are'),\n",
       " (1, 'Testing'),\n",
       " (1, 'Spark](#building-spark).'),\n",
       " (1, 'Once'),\n",
       " (1, 'built,'),\n",
       " (2, 'tests'),\n",
       " (1, 'using:'),\n",
       " (1, './dev/run-tests'),\n",
       " (4, 'Please'),\n",
       " (2, 'guidance'),\n",
       " (1, 'module,'),\n",
       " (1, 'individual'),\n",
       " (1, 'integration'),\n",
       " (1, 'test,'),\n",
       " (1, 'Note'),\n",
       " (1, 'About'),\n",
       " (1, 'uses'),\n",
       " (1, 'library'),\n",
       " (1, 'HDFS'),\n",
       " (1, 'other'),\n",
       " (1, 'Hadoop-supported'),\n",
       " (1, 'storage'),\n",
       " (1, 'systems.'),\n",
       " (1, 'Because'),\n",
       " (1, 'have'),\n",
       " (1, 'changed'),\n",
       " (1, 'different'),\n",
       " (1, 'versions'),\n",
       " (2, 'Hadoop,'),\n",
       " (1, 'must'),\n",
       " (1, 'against'),\n",
       " (1, 'version'),\n",
       " (2, 'refer'),\n",
       " (1,\n",
       "  'YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)'),\n",
       " (2, 'particular'),\n",
       " (1, 'distribution'),\n",
       " (2, 'Hive'),\n",
       " (1, 'Thriftserver'),\n",
       " (1, 'distributions.'),\n",
       " (1, '[Configuration'),\n",
       " (1, 'online'),\n",
       " (1, 'overview'),\n",
       " (1, 'configure'),\n",
       " (1, 'Spark.'),\n",
       " (1, 'Contributing'),\n",
       " (1, 'guide](https://spark.apache.org/contributing.html)'),\n",
       " (1, 'started'),\n",
       " (1, 'contributing'),\n",
       " (1, 'project.'),\n",
       " (12, 'for'),\n",
       " (1, 'large-scale'),\n",
       " (2, 'data'),\n",
       " (2, 'processing.'),\n",
       " (2, 'Python,'),\n",
       " (9, 'and'),\n",
       " (1, 'R,'),\n",
       " (2, 'general'),\n",
       " (1, 'graphs'),\n",
       " (5, 'also'),\n",
       " (1, 'rich'),\n",
       " (1, 'higher-level'),\n",
       " (4, 'including'),\n",
       " (1, 'DataFrames,'),\n",
       " (1, 'Streaming'),\n",
       " (1, 'stream'),\n",
       " (1, '[![Jenkins'),\n",
       " (1,\n",
       "  'Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)'),\n",
       " (1, '[![AppVeyor'),\n",
       " (1, '[![PySpark'),\n",
       " (9, '##'),\n",
       " (1, 'Online'),\n",
       " (3, 'You'),\n",
       " (6, 'can'),\n",
       " (1, 'find'),\n",
       " (1, 'documentation,'),\n",
       " (7, 'on'),\n",
       " (1, 'web'),\n",
       " (1, 'file'),\n",
       " (1, 'contains'),\n",
       " (1, 'setup'),\n",
       " (1, 'built'),\n",
       " (1, 'Maven](https://maven.apache.org/).'),\n",
       " (2, 'To'),\n",
       " (3, 'build'),\n",
       " (1, 'its'),\n",
       " (3, 'example'),\n",
       " (1, 'programs,'),\n",
       " (1, './build/mvn'),\n",
       " (1, '-DskipTests'),\n",
       " (1, 'clean'),\n",
       " (1, 'package'),\n",
       " (1, '(You'),\n",
       " (1, 'need'),\n",
       " (16, 'to'),\n",
       " (4, 'you'),\n",
       " (1, 'pre-built'),\n",
       " (1, 'package.)'),\n",
       " (1, 'More'),\n",
       " (2, 'detailed'),\n",
       " (1, 'available'),\n",
       " (1, 'from'),\n",
       " (1, '[\"Building'),\n",
       " (3, 'For'),\n",
       " (1, 'info'),\n",
       " (3, 'see'),\n",
       " (1, 'Tools\"](https://spark.apache.org/developer-tools.html).'),\n",
       " (2, 'Scala'),\n",
       " (1, 'easiest'),\n",
       " (1, 'through'),\n",
       " (2, 'shell:'),\n",
       " (1, './bin/spark-shell'),\n",
       " (2, 'command,'),\n",
       " (2, 'which'),\n",
       " (2, 'should'),\n",
       " (2, 'return'),\n",
       " (2, '1,000,000,000:'),\n",
       " (2, '1000'),\n",
       " (1, 'prefer'),\n",
       " (1, './bin/pyspark'),\n",
       " (1, '>>>'),\n",
       " (1, 'Programs'),\n",
       " (1, 'comes'),\n",
       " (3, 'with'),\n",
       " (1, 'sample'),\n",
       " (2, '`examples`'),\n",
       " (1, 'directory.'),\n",
       " (2, 'one'),\n",
       " (1, '<class>'),\n",
       " (1, 'will'),\n",
       " (1, 'Pi'),\n",
       " (1, 'locally.'),\n",
       " (1, 'MASTER'),\n",
       " (1, 'environment'),\n",
       " (1, 'running'),\n",
       " (1, 'submit'),\n",
       " (1, 'cluster.'),\n",
       " (1, 'mesos://'),\n",
       " (1, '\"yarn\"'),\n",
       " (1, 'thread,'),\n",
       " (1, '\"local[N]\"'),\n",
       " (1, 'threads.'),\n",
       " (1, 'MASTER=spark://host:7077'),\n",
       " (1, 'Many'),\n",
       " (1, 'given.'),\n",
       " (1, 'Running'),\n",
       " (1, 'Tests'),\n",
       " (1, 'first'),\n",
       " (1, 'requires'),\n",
       " (1, '[building'),\n",
       " (3, 'how'),\n",
       " (1, '[run'),\n",
       " (1,\n",
       "  'tests](https://spark.apache.org/developer-tools.html#individual-tests).'),\n",
       " (1, 'There'),\n",
       " (1, 'Kubernetes'),\n",
       " (1, 'resource-managers/kubernetes/integration-tests/README.md'),\n",
       " (3, 'Hadoop'),\n",
       " (1, 'Versions'),\n",
       " (1, 'core'),\n",
       " (1, 'talk'),\n",
       " (1, 'protocols'),\n",
       " (1, 'same'),\n",
       " (1, 'your'),\n",
       " (1, 'cluster'),\n",
       " (1, 'runs.'),\n",
       " (1, '[\"Specifying'),\n",
       " (1, 'Version'),\n",
       " (1, 'Enabling'),\n",
       " (2, 'building'),\n",
       " (1, 'Configuration'),\n",
       " (1, 'Guide](https://spark.apache.org/docs/latest/configuration.html)'),\n",
       " (1, 'review'),\n",
       " (1, '[Contribution'),\n",
       " (1, 'information'),\n",
       " (1, 'get')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "resultRDD_swap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3. Sắp xếp các key theo thứ tự giảm dần"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 'to')\n",
      "(14, 'Spark')\n",
      "(12, 'for')\n",
      "(9, 'and')\n",
      "(9, '##')\n",
      "(7, 'run')\n",
      "(7, 'on')\n",
      "(6, 'can')\n",
      "(5, 'in')\n",
      "(5, 'of')\n"
     ]
    }
   ],
   "source": [
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "for i in resultRDD_swap_sort.take(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4. Hiển thị 10 từ xuất hiện nhiều nhất trong `resultRDD_swap_sort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to,16\n",
      "Spark,14\n",
      "for,12\n",
      "and,9\n",
      "##,9\n",
      "run,7\n",
      "on,7\n",
      "can,6\n",
      "in,5\n",
      "of,5\n"
     ]
    }
   ],
   "source": [
    "for word in resultRDD_swap_sort.take(10):\n",
    "\tprint(\"{},{}\". format(word[1], word[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3bc1a11519d29b3ed6f08646f3ece60640217e649724c6bcfd38e1173c1a1bce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
